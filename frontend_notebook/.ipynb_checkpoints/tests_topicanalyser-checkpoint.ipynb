{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "735ee3bf-89b6-4560-a095-b8019f4f4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "class TopicAnalyser:\n",
    "    def __init__(self, model_type = \"nmf\", data = None):\n",
    "        self.model_type = model_type\n",
    "        self.data = data\n",
    "\n",
    "    # get the topic analysis of the whole text\n",
    "    def display_topics(self, model, feature_names, no_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            return \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "    def analyse(self):\n",
    "        # TODO: participants should consider changing dataset to match the brief.\n",
    "        if (self.data == None):\n",
    "            dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "            documents = dataset.data\n",
    "\n",
    "        # HYPERPARAMETERS: Consider tuning\n",
    "        no_features = 1000\n",
    "        no_topics = 20\n",
    "\n",
    "        if self.model_type == \"nmf\":\n",
    "            # NMF is able to use tf-idf\n",
    "            tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "            tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "            tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "            # Run NMF\n",
    "            model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "        elif self.model_type == \"lda\":\n",
    "            # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "            tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "            tf = tf_vectorizer.fit_transform(documents)\n",
    "            tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "            # Run LDA\n",
    "            model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online',\n",
    "                                        learning_offset=50., random_state=0).fit(tf)\n",
    "        else:\n",
    "            logging.exception(\"Invalid model_type: {}\".format(self.model_type))\n",
    "\n",
    "        # HYPERPARAMETER: Consider tuning\n",
    "        no_top_words = 20\n",
    "        \n",
    "        topics = self.display_topics(model, tfidf_feature_names, no_top_words)\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d004394a-a62a-40c2-9cb4-23152fa0ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1425: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people time right did good said say make way government point really years going course long believe state fact world\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#t = TopicAnalyser()\n",
    "#t.analyse()\n",
    "\n",
    "# define endpoint url\n",
    "url = \"https://gee15-22-team5.ew.r.appspot.com/api/text\"\n",
    "\n",
    "# use requests library to send HTTP requests\n",
    "# in this example, GET sentiment analysis data\n",
    "#data = json.loads(requests.get(url).text)\n",
    "\n",
    "# examine data\n",
    "#data\n",
    "\n",
    "t = TopicAnalyser()\n",
    "print(t.analyse())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a21a5-d87f-4f5a-8fb5-762a38435e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
